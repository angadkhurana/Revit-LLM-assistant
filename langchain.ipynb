{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the environment which contains the OPENAI_API key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the PDF file\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\angad\\OneDrive\\Desktop\\AML\\thesis\\MSc_Thesis_02298815.pdf\") # PDF file path\n",
    "pages = loader.load_and_split() # essentially splits the pdf into pages with the `page_content` as the text information and `metadata` as the source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.chains import RetrievalQA\n",
    "# from openai import OpenAI\n",
    "import chromadb\n",
    "\n",
    "embeddings = OpenAIEmbeddings() # creating the embeddings\n",
    "llm = ChatOpenAI() # the llm model \n",
    "\n",
    "documents = pages\n",
    "\n",
    "# concatenating all the text in the pdf into one string\n",
    "text = \"\"\n",
    "for i in pages:\n",
    "    text += i.page_content\n",
    "\n",
    "# text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "    )\n",
    "chunks = text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "loader = TextLoader(\"bsc.md\")\n",
    "md_text = loader.load()[0].page_content\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(md_text)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# chunk_size = 1000\n",
    "# chunk_overlap = 500\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "# )\n",
    "\n",
    "# text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=1000,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits\n",
    "chunks = [i.page_content for i in splits]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3677"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vectorstore_bsc', 'rb') as file:\n",
    "    # Use pickle.dump() to serialize and save the object to the file\n",
    "    vectorstore = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# HuggingFace Embeddings\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#     query_instruction=\"Represent the query for retrieval: \"\n",
    "# )\n",
    "\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}) # setting up the text retriever. This allows us to choose the number of documents to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPT4All is a local GPT model, change the model to the location on your PC where the model is installed\n",
    "\n",
    "# from langchain.llms import GPT4All\n",
    "\n",
    "# llm = GPT4All(\n",
    "#     model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\", # change this with your path for GPT4ALL\n",
    "#     max_tokens=2048, # max tokens to generate\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-4') # the llm model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it is violating a guideline. According to the context provided, refuge areas for apartment buildings of height above 60 m while having balconies should be provided at 60 m and thereafter at every 30 m. So if your building is 170m tall, refuge terraces should be provided at heights like 60m, 90m, 120m, and 150m. Your building is missing refuge terraces at 120m and 150m.\n"
     ]
    }
   ],
   "source": [
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    llm=llm # your preferred LLM \n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"I have a residential building that is 170m tall. I have one refuge terrace at 70mts and another at 90 mts. Is this violating any guideline?\"\n",
    "result = qa_chain.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"angad-msc\",embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the text and metadata to chromadb\n",
    "documents = chunks\n",
    "metadatas = [{\"source\":\"angad_msc\"} for i in range(len(chunks)) ]\n",
    "ids = [str(i) for i in range(len(chunks))]\n",
    "collection.add(\n",
    "            documents = documents,\n",
    "            metadatas = metadatas,\n",
    "            ids = ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who is my supervisor?\"\n",
    "question = \"summarise section 3\"\n",
    "results = collection.query(query_texts = [question], n_results = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"In section 3 of the given text, it is stated that the data for the analysis consists of households in the Greater London Area that participated in a Low Carbon London project. The households were divided into two groups, with one group subjected to Dynamic Time of Use (dTOU) prices for the year 2013, while the other group had a flat tariff rate. The analysis focused only on the households with a flat tariff rate, reducing the total number of households from 5567 to 4443. Further filtering was done to consider only the households with complete readings for the entirety of 2013, resulting in a final sample size of 4039 households. Missing values in the data were imputed by averaging the readings one hour before and after the missing values. The data was then aggregated to a one-hour granularity. K-means clustering with k=18 was performed on various metrics related to the households' load profiles. The cluster with the highest number of clients was selected for further analysis.\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "results = collection.query(query_texts = [question], n_results = 2)\n",
    "\n",
    "template = \"\"\"The following piece of text is given: {text}. Please answer any following questions ONLY using THIS piece of text in a brief manner.\"\"\"\n",
    "human_template = \"{question}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() \n",
    "context = ''\n",
    "for i in range(len(results['documents'])):\n",
    "    context = context + results['documents'][0][i] + '\\n'\n",
    "chain.invoke({\"text\": context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
